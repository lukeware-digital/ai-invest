"""
CeciAI - Backup Manager
Sistema de backup robusto para garantir que dados NUNCA sejam perdidos

EstratÃ©gia de Backup (3-2-1):
- 3 cÃ³pias dos dados
- 2 tipos diferentes de mÃ­dia
- 1 cÃ³pia off-site (opcional)

Camadas:
1. DuckDB (primÃ¡rio)
2. Parquet (backup local comprimido)
3. JSON (backup legÃ­vel)
4. Checksum (validaÃ§Ã£o de integridade)

Autor: CeciAI Team
Data: 2025-10-07
"""

import gzip
import hashlib
import json
import os
import shutil
from datetime import datetime
from pathlib import Path

import pandas as pd
from loguru import logger

from utils.database import DatabaseManager


class BackupManager:
    """
    Gerenciador de backups com mÃºltiplas camadas de proteÃ§Ã£o.

    Garante que dados do CoinAPI nunca sejam perdidos.
    """

    def __init__(
        self,
        backup_dir: str = "data/backups",
        enable_compression: bool = True,
        enable_checksums: bool = True,
    ):
        """
        Inicializa o gerenciador de backups.

        Args:
            backup_dir: DiretÃ³rio para backups
            enable_compression: Se True, comprime backups
            enable_checksums: Se True, gera checksums para validaÃ§Ã£o
        """
        self.backup_dir = Path(backup_dir)
        self.enable_compression = enable_compression
        self.enable_checksums = enable_checksums

        # Criar estrutura de diretÃ³rios
        self._create_backup_structure()

        logger.info(f"BackupManager inicializado: {backup_dir}")

    def _create_backup_structure(self):
        """Cria estrutura de diretÃ³rios para backups"""
        dirs = [
            self.backup_dir / "daily",  # Backups diÃ¡rios
            self.backup_dir / "weekly",  # Backups semanais
            self.backup_dir / "monthly",  # Backups mensais
            self.backup_dir / "parquet",  # Backups em Parquet
            self.backup_dir / "json",  # Backups em JSON
            self.backup_dir / "checksums",  # Checksums para validaÃ§Ã£o
        ]

        for dir_path in dirs:
            dir_path.mkdir(parents=True, exist_ok=True)

    # ==================== BACKUP AUTOMÃTICO ====================

    def backup_ohlcv_data(
        self, symbol: str, timeframe: str, df: pd.DataFrame, backup_type: str = "daily"
    ) -> dict[str, str]:
        """
        Faz backup de dados OHLCV em mÃºltiplas camadas.

        Args:
            symbol: Par de trading
            timeframe: Timeframe
            df: DataFrame com dados
            backup_type: Tipo de backup (daily, weekly, monthly)

        Returns:
            Dict com caminhos dos backups criados
        """
        if df.empty:
            logger.warning(f"DataFrame vazio, pulando backup de {symbol} {timeframe}")
            return {}

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_symbol = symbol.replace("/", "_")
        base_name = f"{safe_symbol}_{timeframe}_{timestamp}"

        backups = {}

        # 1. Backup em Parquet (comprimido, eficiente)
        parquet_path = self.backup_dir / "parquet" / f"{base_name}.parquet"
        df.to_parquet(parquet_path, compression="snappy", index=False)
        backups["parquet"] = str(parquet_path)
        logger.info(f"âœ… Backup Parquet: {parquet_path}")

        # 2. Backup em JSON (legÃ­vel, portÃ¡vel)
        json_path = self.backup_dir / "json" / f"{base_name}.json"
        if self.enable_compression:
            json_path = json_path.with_suffix(".json.gz")
            with gzip.open(json_path, "wt", encoding="utf-8") as f:
                df.to_json(f, orient="records", date_format="iso", indent=2)
        else:
            df.to_json(json_path, orient="records", date_format="iso", indent=2)
        backups["json"] = str(json_path)
        logger.info(f"âœ… Backup JSON: {json_path}")

        # 3. Backup periÃ³dico (daily/weekly/monthly)
        periodic_path = self.backup_dir / backup_type / f"{base_name}.parquet"
        shutil.copy2(parquet_path, periodic_path)
        backups["periodic"] = str(periodic_path)
        logger.info(f"âœ… Backup {backup_type}: {periodic_path}")

        # 4. Checksum para validaÃ§Ã£o de integridade
        if self.enable_checksums:
            checksum = self._generate_checksum(df)
            checksum_path = self.backup_dir / "checksums" / f"{base_name}.checksum"

            with open(checksum_path, "w") as f:
                json.dump(
                    {
                        "symbol": symbol,
                        "timeframe": timeframe,
                        "timestamp": timestamp,
                        "rows": len(df),
                        "checksum": checksum,
                        "files": backups,
                    },
                    f,
                    indent=2,
                )

            backups["checksum"] = str(checksum_path)
            logger.info(f"âœ… Checksum: {checksum_path}")

        # 5. Metadados
        self._save_metadata(symbol, timeframe, df, backups)

        return backups

    def backup_database(self, db_path: str = "data/ceciai.duckdb") -> str:
        """
        Faz backup completo do banco DuckDB.

        Args:
            db_path: Caminho do banco DuckDB

        Returns:
            Caminho do backup
        """
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"Banco nÃ£o encontrado: {db_path}")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = self.backup_dir / "daily" / f"ceciai_{timestamp}.duckdb"

        # Copiar banco
        shutil.copy2(db_path, backup_path)

        # Comprimir se habilitado
        if self.enable_compression:
            compressed_path = backup_path.with_suffix(".duckdb.gz")
            with open(backup_path, "rb") as f_in, gzip.open(compressed_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)

            # Remover original nÃ£o comprimido
            backup_path.unlink()
            backup_path = compressed_path

        size_mb = os.path.getsize(backup_path) / (1024 * 1024)
        logger.success(f"âœ… Backup do banco: {backup_path} ({size_mb:.2f} MB)")

        return str(backup_path)

    # ==================== RECUPERAÃ‡ÃƒO ====================

    def restore_from_backup(
        self, backup_path: str, target_path: str | None = None
    ) -> pd.DataFrame:
        """
        Restaura dados de um backup.

        Args:
            backup_path: Caminho do backup
            target_path: Caminho de destino (opcional)

        Returns:
            DataFrame com dados restaurados
        """
        backup_path = Path(backup_path)

        if not backup_path.exists():
            raise FileNotFoundError(f"Backup nÃ£o encontrado: {backup_path}")

        logger.info(f"ğŸ“¥ Restaurando de: {backup_path}")

        # Detectar formato
        if backup_path.suffix == ".parquet":
            df = pd.read_parquet(backup_path)

        elif backup_path.suffix == ".gz":
            if ".json.gz" in backup_path.name:
                with gzip.open(backup_path, "rt", encoding="utf-8") as f:
                    df = pd.read_json(f)
            elif ".duckdb.gz" in backup_path.name:
                # Descomprimir banco
                if target_path:
                    with gzip.open(backup_path, "rb") as f_in, open(target_path, "wb") as f_out:
                        shutil.copyfileobj(f_in, f_out)
                    logger.success(f"âœ… Banco restaurado: {target_path}")
                    return pd.DataFrame()  # Retornar vazio para banco
                else:
                    raise ValueError("target_path necessÃ¡rio para restaurar banco")

        elif backup_path.suffix == ".json":
            df = pd.read_json(backup_path)

        else:
            raise ValueError(f"Formato nÃ£o suportado: {backup_path.suffix}")

        logger.success(f"âœ… Restaurado: {len(df)} linhas")

        # Validar checksum se disponÃ­vel
        if self.enable_checksums:
            self._validate_checksum(backup_path, df)

        return df

    def find_latest_backup(
        self, symbol: str, timeframe: str, backup_type: str = "daily"
    ) -> str | None:
        """
        Encontra o backup mais recente de um sÃ­mbolo.

        Args:
            symbol: Par de trading
            timeframe: Timeframe
            backup_type: Tipo de backup

        Returns:
            Caminho do backup mais recente ou None
        """
        safe_symbol = symbol.replace("/", "_")
        pattern = f"{safe_symbol}_{timeframe}_*.parquet"

        backup_dir = self.backup_dir / backup_type
        backups = list(backup_dir.glob(pattern))

        if not backups:
            return None

        # Ordenar por data (mais recente primeiro)
        backups.sort(reverse=True)

        return str(backups[0])

    # ==================== VALIDAÃ‡ÃƒO ====================

    def _generate_checksum(self, df: pd.DataFrame) -> str:
        """Gera checksum MD5 dos dados"""
        data_str = df.to_json(orient="records", date_format="iso")
        return hashlib.md5(data_str.encode()).hexdigest()

    def _validate_checksum(self, backup_path: Path, df: pd.DataFrame) -> bool:
        """Valida integridade dos dados usando checksum"""
        checksum_path = self.backup_dir / "checksums" / f"{backup_path.stem}.checksum"

        if not checksum_path.exists():
            logger.warning(f"âš ï¸  Checksum nÃ£o encontrado: {checksum_path}")
            return False

        with open(checksum_path) as f:
            metadata = json.load(f)

        expected_checksum = metadata["checksum"]
        actual_checksum = self._generate_checksum(df)

        if expected_checksum == actual_checksum:
            logger.success("âœ… Checksum vÃ¡lido")
            return True
        else:
            logger.error("âŒ Checksum invÃ¡lido! Dados podem estar corrompidos")
            return False

    def _save_metadata(
        self, symbol: str, timeframe: str, df: pd.DataFrame, backups: dict[str, str]
    ):
        """Salva metadados do backup"""
        metadata = {
            "symbol": symbol,
            "timeframe": timeframe,
            "timestamp": datetime.now().isoformat(),
            "rows": len(df),
            "columns": list(df.columns),
            "date_range": {
                "start": df["timestamp"].min().isoformat() if "timestamp" in df else None,
                "end": df["timestamp"].max().isoformat() if "timestamp" in df else None,
            },
            "backups": backups,
        }

        metadata_path = (
            self.backup_dir / "metadata" / f"{symbol.replace('/', '_')}_{timeframe}.json"
        )
        metadata_path.parent.mkdir(exist_ok=True)

        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)

    # ==================== LIMPEZA ====================

    def cleanup_old_backups(
        self, keep_daily: int = 7, keep_weekly: int = 4, keep_monthly: int = 12
    ):
        """
        Remove backups antigos para economizar espaÃ§o.

        Args:
            keep_daily: Manter Ãºltimos N backups diÃ¡rios
            keep_weekly: Manter Ãºltimos N backups semanais
            keep_monthly: Manter Ãºltimos N backups mensais
        """
        logger.info("ğŸ§¹ Limpando backups antigos...")

        def cleanup_dir(dir_path: Path, keep: int):
            backups = sorted(dir_path.glob("*"), reverse=True)

            if len(backups) > keep:
                for backup in backups[keep:]:
                    backup.unlink()
                    logger.info(f"ğŸ—‘ï¸  Removido: {backup.name}")

        cleanup_dir(self.backup_dir / "daily", keep_daily)
        cleanup_dir(self.backup_dir / "weekly", keep_weekly)
        cleanup_dir(self.backup_dir / "monthly", keep_monthly)

        logger.success("âœ… Limpeza concluÃ­da")

    def get_backup_statistics(self) -> dict[str, object]:
        """Retorna estatÃ­sticas dos backups"""
        stats = {"total_backups": 0, "total_size_mb": 0, "by_type": {}}

        for backup_type in ["daily", "weekly", "monthly", "parquet", "json"]:
            dir_path = self.backup_dir / backup_type

            if dir_path.exists():
                files = list(dir_path.glob("*"))
                count = len(files)
                size = sum(f.stat().st_size for f in files if f.is_file())

                stats["by_type"][backup_type] = {"count": count, "size_mb": size / (1024 * 1024)}

                stats["total_backups"] += count
                stats["total_size_mb"] += size / (1024 * 1024)

        return stats


# ==================== INTEGRAÃ‡ÃƒO COM DATABASE ====================


class DatabaseWithBackup(DatabaseManager):
    """
    DatabaseManager com backup automÃ¡tico.

    Todos os dados salvos sÃ£o automaticamente backupeados.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.backup_manager = BackupManager()

    def save_ohlcv_data(self, symbol: str, timeframe: str, df: pd.DataFrame) -> int:
        """Salva dados com backup automÃ¡tico"""
        # Salvar no banco
        rows = super().save_ohlcv_data(symbol, timeframe, df)

        # Backup automÃ¡tico
        try:
            self.backup_manager.backup_ohlcv_data(symbol, timeframe, df)
            logger.info(f"ğŸ’¾ Backup automÃ¡tico criado para {symbol} {timeframe}")
        except Exception as e:
            logger.error(f"âŒ Erro ao criar backup: {e}")

        return rows


# ==================== USAGE EXAMPLE ====================

if __name__ == "__main__":
    print("CeciAI - Backup Manager")
    print("=" * 50)

    # Criar backup manager
    backup_mgr = BackupManager()

    # EstatÃ­sticas
    stats = backup_mgr.get_backup_statistics()
    print("\nğŸ“Š EstatÃ­sticas de Backup:")
    print(f"  Total de backups: {stats['total_backups']}")
    print(f"  Tamanho total: {stats['total_size_mb']:.2f} MB")

    for backup_type, data in stats["by_type"].items():
        print(f"  {backup_type}: {data['count']} backups ({data['size_mb']:.2f} MB)")

    print("\nâœ… Backup Manager funcionando!")
